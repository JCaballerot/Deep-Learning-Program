{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Lab CNN MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCaballerot/Deep_learning_program/blob/main/Deep_learning_program/Modulo_II/Lab_CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn5JENlUtil9"
      },
      "source": [
        "<h1 align=\"center\"><font size=\"5\">APLICACIÓN DE REDES NEURONALES CONVOLUCIONALES</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npfFCE3KRF4Z"
      },
      "source": [
        "Utilizaremos el famoso [MNIST Dataset](https://cocl.us/0uk5s) para construir dos redes neuronales capaces de realizar la clasificación de dígitos escritos a mano. La primera red es un perceptrón multicapa simple (MLP) y la segunda es una red neuronal convolucional (CNN de ahora en adelante). En otras palabras, cuando se le da una entrada, nuestro algoritmo dirá, con algún error asociado, qué tipo de dígito representa esta entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDU3LmFSRF4a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldzSumTZRF4b"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "<font size = 3><strong></strong></font>\n",
        "<br>\n",
        "<h2>Tabla de Contenidos</h2>\n",
        "<ol>\n",
        "    <li><a href=\"#ref1\">¿Qué es Deep Learning?</a></li>\n",
        "    <li><a href=\"#ref2\">Test: ¿Cómo Trabaja TensorFlow?</a></li>\n",
        "    <li><a href=\"#ref3\">Parte 1: Clasifica MNIST usando un modelo simple</a></li>\n",
        "    <li><a href=\"#ref4\">Evaluando el resultado final</a></li>\n",
        "    <li><a href=\"#ref5\">Cómo potenciar el modelo?</a></li>\n",
        "    <li><a href=\"#ref6\">Parte 2: Aplicando Deep Learning en MNIST</a></li>\n",
        "    <li><a href=\"#ref7\">Resumen: Red neuronal convolucional profunda (Deep Convolutional Neural Network)</a></li>\n",
        "    <li><a href=\"#ref8\">Definiendo funciones y entrenando el modelo</a></li>\n",
        "    <li><a href=\"#ref9\">Evaluando el modelo</a></li>\n",
        "</ol>    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZuNU05yRF4d"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMBZnXUyRF4e"
      },
      "source": [
        "<a id=\"ref1\"></a>\n",
        "<h2>¿Qué es Deep Learning?</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4eHKtB2RF4f"
      },
      "source": [
        "<b>Teoría:</b> Deep Learning (también conocido como aprendizaje estructurado profundo, aprendizaje jerárquico o aprendizaje automático profundo) es una rama del aprendizaje automático basada en un conjunto de algoritmos que intentan modelar abstracciones de alto nivel en datos mediante el uso de múltiples capas de procesamiento, con estructuras complejas o de otro tipo, compuesto por múltiples transformaciones no lineales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv-gJD7-RF4g"
      },
      "source": [
        "<img src=\"https://ibm.box.com/shared/static/gcbbrh440604cj2nksu3f44be87b8ank.png\" alt=\"HTML5 Icon\" style=\"width: 600px; height: 450px;\">\n",
        "<div style=\"text-align: center\">Es el momento perfecto para deep learning. Nuestro cerebro no funciona con solo una o tres capas. ¿Por qué sería diferente con las máquinas ? </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXZ0aRqTRF4h"
      },
      "source": [
        "<b>Definiendo \"Deep\":</b> en este contexto, \"deep\" significa que estamos estudiando una red neuronal que tiene varias capas ocultas (más de una), sin importar el tipo (convolucional, agrupación, normalización, totalmente conectada, etc.). La parte más interesante es que algunos artículos notaron que las redes neuronales profundas con las arquitecturas / hiperparámetros correctos logran mejores resultados que las redes neuronales superficiales con el mismo poder computacional (por ejemplo, número de neuronas o conexiones). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygrqBhbQRF4j"
      },
      "source": [
        "<b>En la práctica, definiendo \"Learning\":</b> En el contexto del aprendizaje supervisado, el reconocimiento de dígitos en nuestro caso, la parte de aprendizaje consiste en un target / característica que se va a predecir utilizando un conjunto dado de observaciones con la predicción final ya conocida (etiqueta). En nuestro caso, el objetivo será el dígito (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) y las observaciones son la intensidad y la posición relativa de los píxeles. Después de algún entrenamiento, es posible generar una \"función\" que mapee las entradas (imagen de dígitos) a las salidas deseadas (tipo de dígito). El único problema es qué tan bien se produce esta operación de mapa. Al tratar de generar esta \"función\", el proceso de entrenamiento continúa hasta que el modelo alcanza el nivel deseado de precisión en los datos de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOCyOlpsRF4k"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NphS5fwRF4l"
      },
      "source": [
        "<h2>Instalando TensorFlow </h2>\n",
        "\n",
        "Comenzamos instalando TensorFlow versión 2.2.0 y sus requisitos previos requeridos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv59cCItRF4m"
      },
      "source": [
        "!pip install grpcio==1.24.3\n",
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhpVNuEyRF4v"
      },
      "source": [
        "<b>Considera:</b> Este notebook ha sido creado usando TensorFlow versión 2.2, y podría no trabajar correctamente en otras versiones. Comprueba aquí:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UKjMK6dRF4x"
      },
      "source": [
        "import tensorflow as tf\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
        "\n",
        "\n",
        "if not tf.__version__ >= '2.2.0':\n",
        "    printmd('<<<<<!!!!! ERROR !!!! por favor realice el upgrade para el TensorFlow 2.2.0, o resetear tu Kernel (Kernel->Restart & Clear Output)>>>>>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYL_MNRsRF43"
      },
      "source": [
        "Primero clasificamos MNIST usando un perceptrón multicapa simple y luego, en la segunda parte, usamos el aprendizaje profundo para mejorar la precisión de nuestros resultados.\n",
        "\n",
        "<a id=\"ref3\"></a>\n",
        "<h2>Parte 1: Clasificando MNIST usando un modelo simple.</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJnYJPpcRF43"
      },
      "source": [
        "Vamos a crear un perceptrón multicapa simple, un tipo simple de red neuronal, para realizar tareas de clasificación en el conjunto de datos de dígitos MNIST. Si no está familiarizado con el dataset MNIST, para leer más: <a href=\"https://cocl.us/0uk5s\">click aquí</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMvr_zZ4RF44"
      },
      "source": [
        "<h3>¿Qué es MNIST?</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlfvIZWXRF45"
      },
      "source": [
        "Según el sitio web de LeCun, el MNIST es una: \"dataset de dígitos escritos a mano que tiene un conjunto de entrenamiento de 60,000 ejemplos y un conjunto de prueba de 10,000 ejemplos. Es un subconjunto de un conjunto más grande disponible en NIST. Los dígitos han sido de tamaño normalizado y centrado en una imagen de tamaño fijo \"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ynh7upRF46"
      },
      "source": [
        "Importe el dataset MNIST con la función integrada de TensorFlow</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IucxJDwORF47"
      },
      "source": [
        "Es muy importante notar que MNIST es un dataset altamente optimizado y no contiene imágenes. Deberá crear su propio código si desea ver los dígitos reales. Otra nota al margen importante es el esfuerzo que los autores invirtieron en este dataset con operaciones de normalización y centrado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P54VB93RF47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de816fa0-2231-4eca-d29f-223018124b58"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mx-ofo3xNu8"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8BuuKjZydTB"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzTb-E-PRF4_"
      },
      "source": [
        "Los datos de características están entre 0 y 255, y lo normalizaremos para mejorar el rendimiento de optimización."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ief6Bk4_RF5B"
      },
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUvjLUBXRF5G"
      },
      "source": [
        "Revisemos los primeros valores de etiqueta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_0JR9u-RF5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3d87c1-ece9-4115-8404-41f1b16a4711"
      },
      "source": [
        "print(y_train[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 0 4 1 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kom_kdhiRF5O"
      },
      "source": [
        "El esquema de etiquetas actual simplemente identifica la categoría a la que pertenece cada punto de datos (a cada dígito escrito a mano se le asigna una categoría igual al valor numérico). Necesitamos convertir esto en un one-hot encoded vector. A diferencia de la representación binaria, las etiquetas se presentarán de manera que para representar un número N, the $N^{ésimo}$ bit es 1 mientras que los otros bits ason 0. Por ejemplo, cinco y cero en un código binario serían:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cX8nGBrRF5P"
      },
      "source": [
        "<pre>\n",
        "Representación numérica:    0\n",
        "Binary encoding:        [2^5]  [2^4]   [2^3]   [2^2]   [2^1]   [2^0]  \n",
        "Array/vector:             0      0       0       0       0       0 \n",
        "\n",
        "Representación numérica:    5\n",
        "Binary encoding:        [2^5]  [2^4]   [2^3]   [2^2]   [2^1]   [2^0]  \n",
        "Array/vector:             0      0       0       1       0       1  \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze5k0PpVRF5R"
      },
      "source": [
        "Usando una notación diferente, los mismos dígitos usando la representación de un vector activo se pueden mostrar como:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T_eDVuaRF5S"
      },
      "source": [
        "<pre>\n",
        "Representación numérica:   0\n",
        "One-hot encoding:        [5]   [4]    [3]    [2]    [1]   [0]  \n",
        "Array/vector:             0     0      0      0      0     1   \n",
        "\n",
        "Representación numérica:   5\n",
        "One-hot encoding:        [5]   [4]    [3]    [2]    [1]    [0]  \n",
        "Array/vector:             1     0      0      0      0      0   \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg-dAK3oRF5V"
      },
      "source": [
        "Esta es una operación estándar y se muestra a continuación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS3qiWmzRF5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f68e37-a061-430f-d93e-e8fed7c95f82"
      },
      "source": [
        "print(\"Targets categóricos\")\n",
        "print(y_train[0:5])\n",
        "\n",
        "# Target one hot encoded\n",
        "y_train = tf.one_hot(y_train, 10)\n",
        "y_test = tf.one_hot(y_test, 10)\n",
        "\n",
        "print(\"one hot encoded Target\")\n",
        "print(y_train[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets categóricos\n",
            "[5 0 4 1 9]\n",
            "one hot encoded Target\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(5, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6LG5MM1RF5a"
      },
      "source": [
        "<h3>Analizando la data importada</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHhuaLxgRF5b"
      },
      "source": [
        "Los datos importados se pueden dividir de la siguiente manera:\n",
        "\n",
        "- Training >>  Utilice el conjunto de datos proporcionado con entradas y salidas relacionadas para el entrenamiento de NN. En nuestro caso, si da una imagen que sabe que representa un \"nueve\", este conjunto le dirá a la red neuronal que esperamos un \"nueve\" como salida.  \n",
        "        - 60,000 observaciones\n",
        "        - x_train para los inputs\n",
        "        - y_train para los outputs/target\n",
        "  \n",
        " \n",
        "- Test >> El modelo no tiene acceso a esta información antes de la fase de prueba. Se utiliza para evaluar el rendimiento y la precisión del modelo frente a \"situaciones de la vida real\". No hay más optimización más allá de este punto.\n",
        "        - 10,000 observaciones\n",
        "        - x_test para los inputs\n",
        "        - y_test para los outputs/target\n",
        " \n",
        "- La validación de data no es utilizada en este ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvzaQW8TRF5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e01cf5-15a2-4cf2-f683-ae74c2c6613d"
      },
      "source": [
        "print(\"número de ejemplos de training:\" , x_train.shape[0])\n",
        "print(\"número de ejemplos de tests:\" , x_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "número de ejemplos de training: 60000\n",
            "número de ejemplos de tests: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0VsVhjmRF5g"
      },
      "source": [
        "La nueva API de conjunto de datos en TensorFlow 2.X le permite definir tamaños de lote como parte del conjunto de datos. Esto le permite iterar a través de subconjuntos (batches) de datos durante el entrenamiento. Esta es una práctica común que mejora el rendimiento al calcular gradientes en lotes más pequeños. Veremos esto en acción durante el paso de entrenamiento.\n",
        "\n",
        "Además, puede mezclar el conjunto de datos si cree que hay una distribución sesgada de datos en el conjunto de datos original que puede resultar en lotes con distribuciones diferentes. No estamos mezclando datos aquí."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFGulq6-RF5h"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(50)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnAKsWM8RF5n"
      },
      "source": [
        "<h3>Conversión de una imagen 2D en un vector 1D</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3bPlHDRF5o"
      },
      "source": [
        "Las imágenes MNIST son imágenes cuadradas en miniatura en blanco y negro con 28x28 píxeles. A cada píxel se le asigna una intensidad (originalmente en una escala de 0 a 255). Para que la entrada sea útil para nosotros, necesitamos que estos estén organizados en un vector 1D usando una estrategia consistente, como se muestra en la figura siguiente. Podemos usar Flatten para realizar esta tarea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k47MxCpdRF5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e17a8d7-8e05-4b5e-e3e7-bb074aad832f"
      },
      "source": [
        "# Ejemplo de una clase Flatten y operation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "flatten = Flatten(dtype='float32')\n",
        "\n",
        "\"original data shape\"\n",
        "print(x_train.shape)\n",
        "\n",
        "\"flattened shape\"\n",
        "print(flatten(x_train).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME4CyIAfRF5v"
      },
      "source": [
        "<img src=\"Flatten.png\" alt=\"HTML5 Icon\" style=\"width:350px\"> \n",
        "<div style=\"text-align:center\">Ilutstración de la operación de Flattening</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkF91c6pRF5x"
      },
      "source": [
        "<h3>Asignando bias y pesos a los tensores null </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G45qw4-0RF5x"
      },
      "source": [
        "Ahora vamos a crear los pesos y los sesgos, para ello se utilizarán como matrices llenas de ceros. Los valores que elegimos aquí pueden ser críticos, pero cubriremos una mejor manera en la segunda parte, en lugar de este tipo de inicialización.\n",
        "Dado que estos valores se ajustarán durante el proceso de optimización, los definimos usando `tf.Variable`.\n",
        "\n",
        "NOTA: `tf.Variable` crea variables ajustables que están en el espacio de nombres global, por lo que cualquier función que haga referencia a estas variables no necesita pasar las variables. Pero son globales, ¡cuidado al nombrar!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acygzq5QRF5y"
      },
      "source": [
        "# Peso del tensor\n",
        "W = tf.Variable(tf.zeros([784, 10], tf.float32))\n",
        "\n",
        "# Bias tensor\n",
        "b = tf.Variable(tf.zeros([10], tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Ij9FgbRF54"
      },
      "source": [
        "<h3>Agregando pesos y  Biases al input</h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv2eul7WRF56"
      },
      "source": [
        "La única diferencia para nuestra siguiente operación con respecto a la imagen de abajo es que estamos usando la convención matemática para lo que se está ejecutando en la ilustración. La operación tf.matmul realiza una multiplicación de matrices entre x (inputs) y W (pesos) y, después del código, agrega bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOcSpj10RF57"
      },
      "source": [
        "\n",
        "<img src=\"https://ibm.box.com/shared/static/88ksiymk1xkb10rgk0jwr3jw814jbfxo.png\" alt=\"HTML5 Icon\" style=\"width:350px\"> \n",
        "<div style=\"text-align:center\">Ilustración que muestra cómo se agregan pesos y sesgos a las neuronas / nodos. </div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu3w5tptRF57"
      },
      "source": [
        "def forward(x):\n",
        "    return tf.matmul(x,W) + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3n4jQGVRF6B"
      },
      "source": [
        "<h3> Regression Softmax</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm9c7svfRF6C"
      },
      "source": [
        "Softmax es una función de activación que se utiliza normalmente en problemas de clasificación. Genera las probabilidades de la salida. Por ejemplo, nuestro modelo no estará 100% seguro de que un dígito sea el número nueve, en cambio, la respuesta será una distribución de probabilidades donde, si el modelo es correcto, el número nueve tendrá una probabilidad mayor que los otros dígitos.\n",
        "\n",
        "A modo de comparación, a continuación se muestra el vector one-hot para una etiqueta de nueve dígitos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jckP_tyFRF6D"
      },
      "source": [
        " 0 --> 0  \n",
        " 1 --> 0 \n",
        " 2 --> 0\n",
        " 3 --> 0\n",
        " 4 --> 0\n",
        " 5 --> 0\n",
        " 6 --> 0\n",
        " 7 --> 0\n",
        " 8 --> 0\n",
        " 9 --> 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPktTEPTRF6D"
      },
      "source": [
        "Una máquina no tiene toda esta certeza, por lo que queremos saber cuál es la mejor suposición, pero también queremos entender qué tan segura estaba y cuál era la segunda mejor opción. A continuación se muestra un ejemplo de una distribución hipotética de nueve dígitos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trUOF1CrRF6E"
      },
      "source": [
        " 0 -->0.01  \n",
        " 1 -->0.02  \n",
        " 2 -->0.03  \n",
        " 3 -->0.02  \n",
        " 4 -->0.12  \n",
        " 5 -->0.01  \n",
        " 6 -->0.03\n",
        " 7 -->0.06\n",
        " 8 -->0.10\n",
        " 9 -->0.60 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kW-oLhkRF6F"
      },
      "source": [
        "Softmax es simplemente un exponencial de cada valor de un vector que también está normalizado. La formula es:\n",
        "\n",
        "$$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum{e^{z_i}}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFJC84CNRF6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0132876d-c9b1-4a7c-c869-1620f0792eab"
      },
      "source": [
        "# un ejemplo de cálculo softmax en un vector de entrada\n",
        "vector = [10, 0.2, 8]\n",
        "softmax = tf.nn.softmax(vector)\n",
        "print(\"cálculo softmax\")\n",
        "print(softmax.numpy())\n",
        "print(\"verificando la  normalización\")\n",
        "print(tf.reduce_sum(softmax))\n",
        "print(\"encontrar el vector con mayor valor (label assignment)\")\n",
        "print(\"categoría\", tf.argmax(softmax).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cálculo softmax\n",
            "[8.8075405e-01 4.8839214e-05 1.1919710e-01]\n",
            "verificando la  normalización\n",
            "tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "encontrar el vector con mayor valor (label assignment)\n",
            "categoría 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3pwynbcRF6L"
      },
      "source": [
        "Ahora podemos definir nuestra capa de salida\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToW08gmMRF6L"
      },
      "source": [
        "def activate(x):\n",
        "    return tf.nn.softmax(forward(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07xuMpttRF6P"
      },
      "source": [
        "La salida de la función logística se utiliza para la clasificación entre dos targets 0/1. La función Softmax es un tipo generalizado de función logística. Es decir, Softmax puede generar una distribución de probabilidad categórica multiclase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA8D2REqRF6P"
      },
      "source": [
        "Creemos una función de modelo por conveniencia.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xMI96uWRF6Q"
      },
      "source": [
        "def model(x):\n",
        "    x = flatten(x)\n",
        "    return activate(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zd2eWMuRF6T"
      },
      "source": [
        "<h3>Función de Coste</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6w7gxtoRF6U"
      },
      "source": [
        "Es una función que se utiliza para minimizar la diferencia entre las respuestas correctas (labels) y los resultados estimados por nuestra Red. Aquí usamos la función de entropía cruzada, que es una función de coste popular utilizada para modelos categóricos. La función se define en términos de probabilidades, por lo que debemos utilizar vectores normalizados. Se da como:\n",
        "\n",
        "$$ CrossEntropy = \\sum{y_{Label}\\cdot \\log(y_{Prediction})}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1DiW0VRRF6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc991db1-61fe-41b8-8523-af2f98c6ce44"
      },
      "source": [
        "def cross_entropy(y_label, y_pred):\n",
        "    return (-tf.reduce_sum(y_label * tf.math.log(y_pred + 1.e-10)))\n",
        "# Agregamos 1e-10 para prevenir errores en los cálculos zero\n",
        "\n",
        "# función de coste para modelo no optimizado\n",
        "cross_entropy(y_train, model(x_train)).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138155.1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBb4ZMMCRF6Z"
      },
      "source": [
        "<h3>Tipo de optimización: Descenso Gradiente</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_5TwM-hRF6a"
      },
      "source": [
        "Esta es la parte en la que configura el optimizador para su red neuronal. Hay varios optimizadores disponibles, en nuestro caso usaremos Descenso Gradiente porque es un optimizador bien establecido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0rVaynkRF6b"
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFS8n5MqRF6f"
      },
      "source": [
        "Ahora definimos el paso de entrenamiento. Este paso usa `GradientTape` para calcular automáticamente las derivadas de las funciones que hemos creado manualmente y las aplica usando el optimizador` SGD`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl1Zh_SrRF6f"
      },
      "source": [
        "def train_step(x, y ):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # calculando la función de pérdida\n",
        "        current_loss = cross_entropy( y, model(x))\n",
        "        # calculando el gradiente\n",
        "        #(¡Esto es automático!¡Incluso con funciones especializadas!)\n",
        "        grads = tape.gradient( current_loss , [W,b] )\n",
        "        # Aplicar el SGD a nuestras variables W y b\n",
        "        optimizer.apply_gradients( zip( grads , [W,b] ) )     \n",
        "    return current_loss.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPfe3x2qRF6k"
      },
      "source": [
        "<h3>Training batches</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwHS0YoQRF6l"
      },
      "source": [
        "Entrena usando Gradient Descent en minibatch.\n",
        "\n",
        "En la práctica, Descenso Gradiente Batch no se usa a menudo porque es demasiado costoso computacionalmente. Lo bueno de este método es que tiene el gradiente real, pero con la costosa tarea de computación de usar todo el conjunto de datos de una vez. Debido a este problema, las redes neuronales suelen utilizar minibatch para entrenar.\n",
        "\n",
        "Ya hemos dividido nuestro conjunto de datos completo en lotes de 50 cada uno utilizando la API de conjuntos de datos. Ahora podemos iterar a través de cada uno de esos batches para calcular un gradiente. Una vez que iteramos a través de todos los batches del dataset, completamos un **epoch** o un recorrido completo del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8487Gwa_RF6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a8748e-2521-418a-c40b-41ac9703f0f0"
      },
      "source": [
        "# Para ejecutar esta celda varias veces poner cero a los pesos\n",
        "# Peso del tensor\n",
        "W = tf.Variable(tf.zeros([784, 10],tf.float32))\n",
        "# Bias del tensor\n",
        "b = tf.Variable(tf.zeros([10],tf.float32))\n",
        "\n",
        "loss_values = []\n",
        "accuracies = []\n",
        "epochs = 10\n",
        "\n",
        "for i in range(epochs):\n",
        "    j=0\n",
        "    # cada batch tiene 50 ejemplos\n",
        "    for x_train_batch, y_train_batch in train_ds:\n",
        "        j+=1\n",
        "        current_loss = train_step(x_train_batch, y_train_batch)\n",
        "        if j%500==0: #reportando estadísticas batch intermitentes\n",
        "            print(\"epoch \", str(i), \"batch\", str(j), \"loss:\", str(current_loss) ) \n",
        "    \n",
        "    # recopilación de estadísticas en cada epoch ... función de coste y precisión\n",
        "    # función de coste\n",
        "    current_loss = cross_entropy( y_train, model( x_train )).numpy()\n",
        "    loss_values.append(current_loss)\n",
        "    correct_prediction = tf.equal(tf.argmax(model(x_train), axis=1),\n",
        "                                  tf.argmax(y_train, axis=1))\n",
        "    # presición\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "    accuracies.append(accuracy)\n",
        "    print(\"end of epoch \", str(i), \"loss\", str(current_loss), \"accuracy\", str(accuracy) ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0 batch 500 loss: 321.2568\n",
            "epoch  0 batch 1000 loss: 268.77676\n",
            "end of epoch  0 loss 232957.34 accuracy 0.80016667\n",
            "epoch  1 batch 500 loss: 191.31088\n",
            "epoch  1 batch 1000 loss: 173.05966\n",
            "end of epoch  1 loss 217268.58 accuracy 0.8214667\n",
            "epoch  2 batch 500 loss: 219.25847\n",
            "epoch  2 batch 1000 loss: 158.98907\n",
            "end of epoch  2 loss 210818.14 accuracy 0.82808334\n",
            "epoch  3 batch 500 loss: 157.10153\n",
            "epoch  3 batch 1000 loss: 138.40044\n",
            "end of epoch  3 loss 232570.4 accuracy 0.80981666\n",
            "epoch  4 batch 500 loss: 214.29918\n",
            "epoch  4 batch 1000 loss: 182.41684\n",
            "end of epoch  4 loss 219563.36 accuracy 0.82241666\n",
            "epoch  5 batch 500 loss: 225.64648\n",
            "epoch  5 batch 1000 loss: 173.13219\n",
            "end of epoch  5 loss 210040.0 accuracy 0.83205\n",
            "epoch  6 batch 500 loss: 183.22713\n",
            "epoch  6 batch 1000 loss: 174.01784\n",
            "end of epoch  6 loss 199566.73 accuracy 0.8423\n",
            "epoch  7 batch 500 loss: 211.75125\n",
            "epoch  7 batch 1000 loss: 182.0682\n",
            "end of epoch  7 loss 206548.44 accuracy 0.83536667\n",
            "epoch  8 batch 500 loss: 166.04149\n",
            "epoch  8 batch 1000 loss: 153.58104\n",
            "end of epoch  8 loss 202008.19 accuracy 0.8395333\n",
            "epoch  9 batch 500 loss: 217.27345\n",
            "epoch  9 batch 1000 loss: 175.25919\n",
            "end of epoch  9 loss 199146.67 accuracy 0.8420333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2OGJQbPRF6q"
      },
      "source": [
        "<h3>Test y Gráficos</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QekQAacgRF6r"
      },
      "source": [
        "Es común ejecutar diagnósticos intermitentes (como precisión y pérdida en todo el dataset) durante el entrenamiento. Aquí también calculamos una estadística de resumen en el dataset de prueba. Las métricas de ajuste para los datos de entrenamiento deben coincidir estrechamente con las de los datos de la prueba. Si las métricas de prueba son claramente menos favorables, esto puede ser una señal de overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo2L307HRF6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af960edd-b2b6-4031-8f61-d1ab190145e6"
      },
      "source": [
        "correct_prediction_train = tf.equal(tf.argmax(model(x_train), axis=1),tf.argmax(y_train,axis=1))\n",
        "accuracy_train = tf.reduce_mean(tf.cast(correct_prediction_train, tf.float32)).numpy()\n",
        "\n",
        "correct_prediction_test = tf.equal(tf.argmax(model(x_test), axis=1),tf.argmax(y_test, axis=1))\n",
        "accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32)).numpy()\n",
        "\n",
        "print(\"training accuracy\", accuracy_train)\n",
        "print(\"test accuracy\", accuracy_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training accuracy 0.8420333\n",
            "test accuracy 0.8369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLjU0Nd3RF6v"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uiLs8AWRF6v"
      },
      "source": [
        "Los dos gráficos siguientes muestran el rendimiento de la optimización en cada **epoch**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz07v0DPRF6w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "8c47ec57-1404-4dee-e775-7ac7cd3ff385"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "#print(loss_values)\n",
        "plt.plot(loss_values,'-ro')\n",
        "plt.title(\"pérdida por epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"pérdida\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'pérdida')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrNK8iubRF61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "b2e186ae-4afb-4995-d675-01b788dc5ed5"
      },
      "source": [
        "plt.plot(accuracies,'-ro')\n",
        "plt.title(\"precisión per epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"precisión\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'precisión')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGwiozK7RF65"
      },
      "source": [
        "<a id=\"ref4\"></a>\n",
        "<h2>Evaluando el resultado final</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nJnaZ33RF65"
      },
      "source": [
        "\n",
        "La precisión del 84% no está mal considerando la simplicidad del modelo, pero en el pasado se logró una precisión> 90%.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1tRFx4HRF66"
      },
      "source": [
        "<a id=\"ref5\"></a>\n",
        "<h2>¿Cómo potenciar nuestro modelo?</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R75WdinRF67"
      },
      "source": [
        "<h4>Tenemos varias opciones:</h4>\n",
        "<ul>\n",
        "    <li>Regularización de redes neuronales mediante DropConnect</li>\n",
        "    <li>Redes neuronales profundas de varias columnas para clasificación de imágenes</li> \n",
        "    <li>APAC: Clasificación de patrones aumentada con redes neuronales</li>\n",
        "    <li>Red neuronal profunda simple con dropout</li>\n",
        "</ul>\n",
        "<h4>En la siguiente parte vamos a explorar la opción:</h4>\n",
        "<ul>\n",
        "    <li>Red neuronal profunda simple con deserción (más de 1 capa oculta)</li>\n",
        "</ul> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tE5wmtQRF68"
      },
      "source": [
        "<a id=\"ref6\"></a>\n",
        "<h2>Parte 2: Aplicando Deep Learning en MNIST</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDc7D3TKRF69"
      },
      "source": [
        "En la primera parte, aprendimos cómo usar un ANN simple para clasificar MNIST. Ahora vamos a ampliar nuestro conocimiento utilizando una red neuronal profunda.\n",
        "La arquitectura de nuestra red es:\n",
        "    \n",
        "- (Input) -> [batch_size, 28, 28, 1]  >> Aplicando 32 filtros de [5x5]\n",
        "- (Capa Convolucional 1)  -> [batch_size, 28, 28, 32]\n",
        "- (ReLU 1)  -> [?, 28, 28, 32]\n",
        "- (Max pooling 1) -> [?, 14, 14, 32]\n",
        "- (Capa Convolucional 2)  -> [?, 14, 14, 64] \n",
        "- (ReLU 2)  -> [?, 14, 14, 64] \n",
        "- (Max pooling 2)  -> [?, 7, 7, 64] \n",
        "- [capa fully connected 3] -> [1x1024]\n",
        "- [ReLU 3]  -> [1x1024]\n",
        "- [Drop out]  -> [1x1024]\n",
        "- [capa fully connected 4] -> [1x10]\n",
        "\n",
        "\n",
        "Las próximas células explorarán esta nueva arquitectura."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7TnTTIyRF69"
      },
      "source": [
        "<h3>The MNIST data</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "YLCyNAZXRF6-"
      },
      "source": [
        "El dataset MNIST se utilizará del ejemplo anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcEeL-aFRF6_"
      },
      "source": [
        "<h3>Parametros Iniciales</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMLHTXq0RF6_"
      },
      "source": [
        "Crea parámetros generales para el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfLuLXRrRF7A"
      },
      "source": [
        "width = 28 # ancho de la imagen en píxeles\n",
        "height = 28 # altura de la imagen en píxeles\n",
        "flat = width * height # número de píxeles en una imagen\n",
        "class_output = 10 # número de posibles clasificaciones para el problema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJFISLoeRF7F"
      },
      "source": [
        "<h4>Conversión de imágenes del dataset a tensores</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWCYNYaZRF7F"
      },
      "source": [
        "La imagen de entrada es de 28 píxeles por 28 píxeles, 1 canal (escala de grises). En este caso, la primera dimensión es el <b>número de batch</b> de la imagen, y puede ser de cualquier tamaño (así que lo configuramos en -1). La segunda y tercera dimensión son el ancho y el alto, y la última son los canales de imagen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctrI19JORF7G"
      },
      "source": [
        "x_image_train = tf.reshape(x_train, [-1,28,28,1])  \n",
        "x_image_train = tf.cast(x_image_train, 'float32') \n",
        "\n",
        "x_image_test = tf.reshape(x_test, [-1,28,28,1]) \n",
        "x_image_test = tf.cast(x_image_test, 'float32') \n",
        "\n",
        "#creando un nuevo conjunto de datos con un reshape de entradas \n",
        "train_ds2 = tf.data.Dataset.from_tensor_slices((x_image_train, y_train)).batch(50)\n",
        "test_ds2 = tf.data.Dataset.from_tensor_slices((x_image_test, y_test)).batch(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgcKPSKoRF7L"
      },
      "source": [
        "Reducir el tamaño del conjunto de datos a partir de este momento porque Skills Netowrk Labs solo proporciona 4 GB de memoria principal, pero de lo contrario se necesitan 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfoO1s59Cfnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d831fc0-911d-4308-d68c-fbfd1a002965"
      },
      "source": [
        "x_image_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 28, 28, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S4b_2jzRF7L"
      },
      "source": [
        "x_image_train = tf.slice(x_image_train, [0,0,0,0], [10000, 28, 28, 1])\n",
        "y_train = tf.slice(y_train,[0,0],[10000, 10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0it2PLXRF7P"
      },
      "source": [
        "<h3>Capa Convolucional 1</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbIwLLV0RF7P"
      },
      "source": [
        "<h4>Definiendo el peso del kernel y el bias</h4>\n",
        "Definimos un kernel aquí. El tamaño del filtro / kernel es 5x5; Los canales de entrada son 1 (escala de grises); y necesitamos 32 mapas de características diferentes (aquí, 32 mapas de características significa que se aplican 32 filtros diferentes en cada imagen. Por lo tanto, la salida de la capa de convolución sería 28x28x32). En este paso, creamos un tensor de forma de filtro / núcleo <code>[filter_height, filter_width, in_channels, out_channels]</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWK8Jw5ARF7Q"
      },
      "source": [
        "W_conv1 = tf.Variable(tf.random.truncated_normal([5, 5, 1, 32], stddev = 0.1, seed=0))\n",
        "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32])) # necesitamos 32 outputs y 32 biases."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnFUmZEHRF7T"
      },
      "source": [
        "<img src=\"https://ibm.box.com/shared/static/vn26neef1nnv2oxn5cb3uueowcawhkgb.png\" style=\"width: 800px; height: 400px;\" alt=\"HTML5 Icon\" >\n",
        "\n",
        "</h4>Convolucionar el peso del tensor and agregando biases.</h4>\n",
        "\n",
        "Para crear una capa convolucional, usamos <b>tf.nn.conv2d</b>. Calcula una convolución 2-D dada la entrada 4-D y los tensores de filtro.\n",
        "\n",
        "Inputs:\n",
        "- tensor de forma [batch, in_height, in_width, in_channels]. x shape [batch_size,28 ,28, 1]\n",
        "- un filtro / tensor de forma del tensor [filter_height, filter_width, in_channels, out_channels]. W es de tamaño [5, 5, 1, 32]\n",
        "- stride which is  [1, 1, 1, 1]. La capa convolucional desliza la \"ventana del tensor\" a través del tensor de entrada. Como el tensor de entrada tiene 4 dimensiones:  [batch, height, width, channels], luego la convolución opera en una ventana 2D en las dimensiones de alto y ancho. __strides__ determina cuánto se desplaza la ventana en cada una de las dimensiones. Como la primera y la última dimensión están relacionadas con el lote y los canales, establecemos el paso en 1. Pero para la segunda y la tercera dimensión, podríamos establecer otros valores, por Ej. [1, 2, 2, 1]\n",
        "    \n",
        "    \n",
        "Proceso:\n",
        "- Cambie el filtro a una matriz 2-D con forma [5\\*5\\*1,32]\n",
        "- Extrae parches de imagen del tensor de entrada para formar un tensor de forma *virtual* `[batch, 28, 28, 5*5*1]`.\n",
        "- Para cada batch, multiplica a la derecha la matriz de filtro y el vector de imagen.\n",
        "\n",
        "Output:\n",
        "- Un `Tensor` (una convolución 2-D) de tamaño tf.Tensor 'add_7:0' shape=(?, 28, 28, 32)- Nota: la salida de la primera capa de convolución es 32 [28x28] imágenes. Aquí 32 se considera como volumen/profundidad de la imagen de salida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SUMNoffRF7U"
      },
      "source": [
        "def convolve1(x):\n",
        "    return(tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCvgX9ueRF7b"
      },
      "source": [
        "<img src=\"https://ibm.box.com/shared/static/iizf4ui4b2hh9wn86pplqxu27ykpqci9.png\" style=\"width: 800px; height: 400px;\" alt=\"HTML5 Icon\" >\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KfRDlAQRF7c"
      },
      "source": [
        "<h4>Aplicando la función de activación ReLU</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co111AzkRF7d"
      },
      "source": [
        "En este paso, solo pasamos por la capa de convolución de todas las salidas, <b>convolve1</b>, y siempre que aparece un número negativo, lo cambiamos por un 0. Se llama Función de activación de ReLU.<br> Sea f (x) una función de activación de ReLU $f(x) = max(0,x)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRkeWcGjRF7g"
      },
      "source": [
        "def h_conv1(x): \n",
        "  return(tf.nn.relu(convolve1(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh9eDMiQRF7l"
      },
      "source": [
        "<h4>Aplicando max pooling</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5VJjPMTRF7n"
      },
      "source": [
        "<b>max pooling</b> es una forma de muestreo descendente no lineal. Divide la imagen de entrada en un conjunto de rectángulos y, luego, encuentra el valor máximo para esa región.\n",
        "\n",
        "Usemos <b>tf.nn.max_pool</b> función para realizar la agrupación máxima.\n",
        "<b>Tamaño del Kernel:</b> 2x2 (si la ventana es una matriz de 2x2, resultaría en un píxel de salida)  \n",
        "<b>Strides:</b> dicta el comportamiento deslizante del kernel. En este caso, se moverá 2 píxeles cada vez, por lo que no se superpondrá. La entrada es una matriz de tamaño 28x28x32 y la salida sería una matriz de tamaño 14x14x32.\n",
        "\n",
        "<img src=\"https://ibm.box.com/shared/static/kmaja90mn3aud9mro9cn8pbbg1h5pejy.png\" alt=\"HTML5 Icon\" style=\"width: 800px; height: 400px;\"> \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MRL9_HiRF7n"
      },
      "source": [
        "def conv1(x):\n",
        "    return tf.nn.max_pool(h_conv1(x), ksize=[1, 2, 2, 1], \n",
        "                          strides=[1, 2, 2, 1], padding='SAME')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5721P43aRF7s"
      },
      "source": [
        "Primera capa completada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Av6amUrRF7t"
      },
      "source": [
        "<h3>Capa convolucional 2</h3>\n",
        "<h4>Pesos y biases del Kernel</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jje3bXdyRF7u"
      },
      "source": [
        "Aplicamos la convolución nuevamente en esta capa. Veamos el núcleo de la segunda capa:\n",
        "- Filtro / kernel: 5x5 (25 píxeles)\n",
        "- Canales de entrada: 32 (desde la primera capa Conv, teníamos 32 mapas de características)\n",
        "- 64 mapas de características de salida\n",
        "\n",
        "<b>Nota:</b> aquí, la imagen de entrada es [14x14x32], el filtro es [5x5x32], usamos 64 filtros de tamaño [5x5x32] y la salida de la capa convolucional sería 64 imágenes convolucionadas, [14x14x64].\n",
        "\n",
        "<b>Nota:</b> el resultado de la convolución de aplicar un filtro de tamaño [5x5x32] sobre una imagen de tamaño [14x14x32] es una imagen de tamaño [14x14x1], es decir, la convolución funciona en volumen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6iPzZBmRF7v"
      },
      "source": [
        "W_conv2 = tf.Variable(tf.random.truncated_normal([5, 5, 32, 64], stddev=0.1, seed=1))\n",
        "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64])) #necesita 64 biases para 64 outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dciu0DjRF7z"
      },
      "source": [
        "<h4>Convierta la imagen con el tensor de peso y agregue biases.</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndDvjUHBRF7z"
      },
      "source": [
        "def convolve2(x): \n",
        "    return( \n",
        "    tf.nn.conv2d(conv1(x), W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r09ZNUSaRF75"
      },
      "source": [
        "<h4>Aplicando la función de activación ReLU</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32jlvfqPRF77"
      },
      "source": [
        "def h_conv2(x):  return tf.nn.relu(convolve2(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCxCc5tRF79"
      },
      "source": [
        "<h4>Aplicando max pooling</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpx86TRjRF7_"
      },
      "source": [
        "def conv2(x):  \n",
        "    return(\n",
        "    tf.nn.max_pool(h_conv2(x), ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpsa1v9wRF8D"
      },
      "source": [
        "Segunda capa completada. Entonces, ¿cuál es la salida de la segunda capa, capa 2?\n",
        "- es una matriz de 64 de [7x7]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nSVqr5SRF8D"
      },
      "source": [
        "<h3>Capa Fully Connected</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIeSX5XfRF8F"
      },
      "source": [
        "Necesita una capa completamente conectada para usar Softmax y crear las probabilidades al final. Las capas completamente conectadas toman las imágenes filtradas de alto nivel de la capa anterior, es decir, las 64 matrices, y las convierten en una matriz plana.\n",
        "\n",
        "Por tanto, cada matriz [7x7] se convertirá en una matriz de [49x1], y luego se conectarán todas las 64 matrices, lo que hará una matriz de tamaño [3136x1]. Lo conectaremos a otra capa de tamaño [1024x1]. Por lo tanto, el peso entre estas 2 capas será [3136x1024]\n",
        "\n",
        "\n",
        "<img src=\"https://ibm.box.com/shared/static/pr9mnirmlrzm2bitf1d4jj389hyvv7ey.png\" alt=\"HTML5 Icon\" style=\"width: 800px; height: 400px;\"> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqY6n0vpRF8F"
      },
      "source": [
        "<h4>Flattening en la segunda Capa</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJKZLe_tRF8G"
      },
      "source": [
        "def layer2_matrix(x): \n",
        "  return tf.reshape(conv2(x), [-1, 7 * 7 * 64])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLMRGQuTRF8J"
      },
      "source": [
        "<h4>Pesos y Biases entre la capa 2 y 3</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayVt-dkuRF8K"
      },
      "source": [
        "Composición del mapa de características de la última capa (7x7) multiplicado por el número de mapas de características (64); 1027 salidas a la capa Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb6TIyshRF8L"
      },
      "source": [
        "W_fc1 = tf.Variable(tf.random.truncated_normal([7 * 7 * 64, 1024], stddev=0.1, seed = 2))\n",
        "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024])) # necesita 1024 biases para 1024 outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq5X2JKfRF8O"
      },
      "source": [
        "<h4>Multiplicación de matrices (aplicando ponderaciones y bias)</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D27lLFY4RF8P"
      },
      "source": [
        "def fcl(x): return tf.matmul(layer2_matrix(x), W_fc1) + b_fc1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEp_Q8QwRF8U"
      },
      "source": [
        "<h4>Aplicando la función de activación ReLU</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDzFidFWRF8V"
      },
      "source": [
        "def h_fc1(x): return tf.nn.relu(fcl(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wWkwvKrRF8X"
      },
      "source": [
        "Tercera capa completada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6BGROIJRF8d"
      },
      "source": [
        "<h4>Capa de dropout, fase opcional para reducir overfitting</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imrleHn6RF8d"
      },
      "source": [
        "Es una fase en la que la red \"olvida\" algunas características. En cada paso de entrenamiento en un mini-lote, algunas unidades se apagan aleatoriamente para que no interactúen con la red. Es decir, sus ponderaciones no se pueden actualizar ni afectar el aprendizaje de los otros nodos de la red. Esto puede resultar muy útil para redes neuronales muy grandes para evitar el sobreajuste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9r1t9YrRF8e"
      },
      "source": [
        "keep_prob=0.5\n",
        "def layer_drop(x): \n",
        "  return tf.nn.dropout(h_fc1(x), keep_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0emfAMY6RF8h"
      },
      "source": [
        "<h4>Capa de Lectura (Softmax Layer)</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jgacqpXRF8i"
      },
      "source": [
        "Type: Softmax, Capa Fully Connected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNMB1WBfRF8i"
      },
      "source": [
        "<h4>Pesos y Biases</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEDedqeoRF8l"
      },
      "source": [
        "En la última capa, CNN toma las imágenes filtradas de alto nivel y las traduce en votos usando softmax.\n",
        "Canales de entrada: 1024 (neuronas de la 3ª capa); 10 funciones de salida"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJgBJlIVRF8l"
      },
      "source": [
        "W_fc2 = tf.Variable(tf.random.truncated_normal([1024, 10], stddev=0.1, seed = 2)) #1024 neuronas\n",
        "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10])) # 10 posibilidades por dígitos [0,1,2,3,4,5,6,7,8,9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRFbjezURF8n"
      },
      "source": [
        "<h4>Multiplicación de matrices (aplicando pesos y biases)</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPbwt8RsRF8o"
      },
      "source": [
        "def fc(x): return tf.matmul(layer_drop(x), W_fc2) + b_fc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c3kIsyzRF8r"
      },
      "source": [
        "<h4>Aplicando la función de activación Softmax</h4>\n",
        "<b>softmax</b> nos permite interpretar las salidas de <b>fcl4</b> como probabilidades. Entonces, <b>y_conv</b> es un tensor de probabilidades."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q60AW1jgRF8s"
      },
      "source": [
        "def y_CNN(x): return tf.nn.softmax(fc(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDdo_ckfRF8w"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQPSbF9yRF8x"
      },
      "source": [
        "<a id=\"ref7\"></a>\n",
        "<h2>Resumen Red Neuronal convolucional profunda</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpFuh9KgRF8y"
      },
      "source": [
        "Ahora es el momento de recordar la estructura de nuestra red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAUYsrNDRF8y"
      },
      "source": [
        "#### 0) Input - MNIST dataset\n",
        "#### 1) Convolucional y Max-Pooling\n",
        "#### 2) Convolucional y Max-Pooling\n",
        "#### 3) Capa Fully Connected\n",
        "#### 4) Procesando - Dropout\n",
        "#### 5) Capa de Lectura - Fully Connected\n",
        "#### 6) Outputs - Classificando dígitos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP3RHNElRF8z"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaFX2V9SRF8z"
      },
      "source": [
        "<a id=\"ref8\"></a>\n",
        "<h2>Definir funciones y entrenar el modelo</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSFUNrZERF8z"
      },
      "source": [
        "<h4>Definir la función de coste</h4>\n",
        "\n",
        "Necesitamos comparar nuestra salida, tensor de capa4, con la verdad fundamental para todos los mini_batch. nosotros podemos usar <b>cross entropy</b> para ver qué tan mal está funcionando nuestra CNN, para medir el error en una capa softmax.\n",
        "\n",
        "El siguiente código muestra una muestra de juguete de entropía cruzada para un mini lote de tamaño 2 en el que se han clasificado sus artículos. Puede ejecutarlo (primero cambie el tipo de celda a <b>code</b> en la barra de herramientas) para ver cómo cambia la entropía cruzada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_PpdWsHRF80"
      },
      "source": [
        "import numpy as np\n",
        "layer4_test =[[0.9, 0.1, 0.1],[0.9, 0.1, 0.1]]\n",
        "y_test=[[1.0, 0.0, 0.0],[1.0, 0.0, 0.0]]\n",
        "np.mean( -np.sum(y_test * np.log(layer4_test),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoswudSFRF81"
      },
      "source": [
        "<b>reduce_sum</b> calcula la suma de elementos de<b>(y_ * tf.log(layer4)</b> a través de la segunda dimensión del tensor, y <b>reduce_mean</b> calcula la media de todos los elementos en el tensor.\n",
        "\n",
        "$$ CrossEntropy = \\sum{y_{Label}\\cdot \\log(y_{Prediction})}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIqxgTAcRF81"
      },
      "source": [
        "def cross_entropy(y_label, y_pred):\n",
        "    return (-tf.reduce_sum(y_label * tf.math.log(y_pred + 1.e-10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4g_cXntRF85"
      },
      "source": [
        "<h4>Definir el optimizador</h4>\n",
        "\n",
        "Es obvio que queremos minimizar el error de nuestra red, que se calcula mediante la métrica cross_entropy. Para resolver el problema, tenemos que calcular gradientes para la pérdida (que minimiza la entropía cruzada) y aplicar gradientes a las variables. Lo hará un optimizador: Descenso Gradiente o Adagrad. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49UR3urMRF86"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjQPcD-KRF9A"
      },
      "source": [
        "Siguiendo la convención de nuestro primer ejemplo, usaremos `GradientTape` para definir un modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNj-liQDRF9B"
      },
      "source": [
        "variables = [W_conv1, b_conv1, W_conv2, b_conv2, \n",
        "             W_fc1, b_fc1, W_fc2, b_fc2, ]\n",
        "\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = cross_entropy( y, y_CNN( x ))\n",
        "        grads = tape.gradient( current_loss , variables )\n",
        "        optimizer.apply_gradients( zip( grads , variables ) )\n",
        "        return current_loss.numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOU45yysRF9G"
      },
      "source": [
        "\"\"\"results = []\n",
        "increment = 1000\n",
        "for start in range(0,60000,increment):\n",
        "    s = tf.slice(x_image_train,[start,0,0,0],[start+increment-1, 28, 28, 1])\n",
        "    t = y_CNN(s)\n",
        "    #results.append(t)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOGKGo4sRF9J"
      },
      "source": [
        "<h4>Definir predicción</h4>\n",
        "¿Quiere saber cuántos de los casos de un mini lote se han clasificado correctamente? vamos a contarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_HCBoyfRF9J"
      },
      "source": [
        "correct_prediction = tf.equal(tf.argmax(y_CNN(x_image_train), axis=1), tf.argmax(y_train, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_wBusrmRF9L"
      },
      "source": [
        "<h4>Definir precisión</h4>\n",
        "Tiene más sentido informar la precisión utilizando el promedio de casos correctos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2_7euJ7RF9M"
      },
      "source": [
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float32'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQJqDeGKRF9P"
      },
      "source": [
        "<h4>Sesión Run, train</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "256apeOtRF9R"
      },
      "source": [
        "<i>Si quieres un resultado rápido (<b>puede llevar algún tiempo entrenarlo</b>)</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9krEnhQARF9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a42c76-2d7a-43f4-a01f-f84096c2b340"
      },
      "source": [
        "loss_values=[]\n",
        "accuracies = []\n",
        "epochs = 1\n",
        "\n",
        "for i in range(epochs):\n",
        "    j=0\n",
        "    # cada batch tiene 50 ejemplos\n",
        "    for x_train_batch, y_train_batch in train_ds2:\n",
        "        j+=1\n",
        "        current_loss = train_step(x_train_batch, y_train_batch)\n",
        "        if j%50==0: #reportar estadísticas de batch intermitentes\n",
        "            correct_prediction = tf.equal(tf.argmax(y_CNN(x_train_batch), axis=1),\n",
        "                                  tf.argmax(y_train_batch, axis=1))\n",
        "            #  precisión\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "            print(\"epoch \", str(i), \"batch\", str(j), \"loss:\", str(current_loss),\n",
        "                     \"accuracy\", str(accuracy)) \n",
        "            \n",
        "    current_loss = cross_entropy( y_train, y_CNN( x_image_train )).numpy()\n",
        "    loss_values.append(current_loss)\n",
        "    correct_prediction = tf.equal(tf.argmax(y_CNN(x_image_train), axis=1),\n",
        "                                  tf.argmax(y_train, axis=1))\n",
        "    #  precisión\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "    accuracies.append(accuracy)\n",
        "    print(\"end of epoch \", str(i), \"loss\", str(current_loss), \"accuracy\", str(accuracy) )  \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0 batch 50 loss: 15.607309 accuracy 0.94\n",
            "epoch  0 batch 100 loss: 21.425518 accuracy 0.88\n",
            "epoch  0 batch 150 loss: 11.453228 accuracy 0.94\n",
            "epoch  0 batch 200 loss: 7.4433417 accuracy 0.96\n",
            "epoch  0 batch 250 loss: 14.979427 accuracy 0.8\n",
            "epoch  0 batch 300 loss: 12.622277 accuracy 0.92\n",
            "epoch  0 batch 350 loss: 20.197962 accuracy 0.94\n",
            "epoch  0 batch 400 loss: 6.409218 accuracy 0.92\n",
            "epoch  0 batch 450 loss: 16.453423 accuracy 0.86\n",
            "epoch  0 batch 500 loss: 10.363175 accuracy 0.96\n",
            "epoch  0 batch 550 loss: 10.632949 accuracy 0.96\n",
            "epoch  0 batch 600 loss: 14.362846 accuracy 0.9\n",
            "epoch  0 batch 650 loss: 12.041695 accuracy 0.94\n",
            "epoch  0 batch 700 loss: 5.6133795 accuracy 0.98\n",
            "epoch  0 batch 750 loss: 21.887321 accuracy 0.9\n",
            "epoch  0 batch 800 loss: 11.9798975 accuracy 0.84\n",
            "epoch  0 batch 850 loss: 9.148705 accuracy 0.96\n",
            "epoch  0 batch 900 loss: 8.81819 accuracy 0.96\n",
            "epoch  0 batch 950 loss: 10.392128 accuracy 0.9\n",
            "epoch  0 batch 1000 loss: 15.65193 accuracy 0.92\n",
            "epoch  0 batch 1050 loss: 4.9506836 accuracy 0.96\n",
            "epoch  0 batch 1100 loss: 12.601755 accuracy 0.94\n",
            "epoch  0 batch 1150 loss: 9.327814 accuracy 0.9\n",
            "epoch  0 batch 1200 loss: 4.480837 accuracy 0.98\n",
            "end of epoch  0 loss 1823.8534 accuracy 0.9435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMvtcLoeRF9U"
      },
      "source": [
        "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
        "<font size = 3><strong><i>¡95% de precisión después de solo 1 epoch! Puede aumentar el número de epoch en la celda anterior si REALMENTE tiene tiempo para esperar, o la está ejecutando usando PowerAI (<b> cambie el tipo de celda a código</b>)</i></strong></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhkQUpZWRF9V"
      },
      "source": [
        "<i>PD. Si tiene problemas para ejecutar este portátil, apague todos sus portátiles en ejecución de Jupyter, borre todas las salidas de las celdas y ejecute cada celda solo después de completar la celda anterior.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0AtR3AVRF9V"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmbbXkAJRF9V"
      },
      "source": [
        "<a id=\"ref9\"></a>\n",
        "<h2>Evalua el modelo</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t7o2StaRF9W"
      },
      "source": [
        "Print de la evaluación para el usuario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGhKt8THRF9W"
      },
      "source": [
        "j=0\n",
        "acccuracies=[]\n",
        "# evaluar la precisión por batch y promedio ... informar cada 100 batch\n",
        "for x_train_batch, y_train_batch in train_ds2:\n",
        "        j+=1\n",
        "        correct_prediction = tf.equal(tf.argmax(y_CNN(x_train_batch), axis=1),\n",
        "                                  tf.argmax(y_train_batch, axis=1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)).numpy()\n",
        "        #accuracies.append(accuracy)\n",
        "        if j%100==0:\n",
        "            print(\"batch\", str(j), \"accuracy\", str(accuracy) ) \n",
        "import numpy as np\n",
        "print(\"precisión de todo el conjunto\", str(np.mean(accuracies)))            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZF-cU_ERF9Y"
      },
      "source": [
        "<h3>Visualización</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKYpg__cRF9Y"
      },
      "source": [
        "¿Quieres ver todos los filtros?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8zU9_orRF9Z"
      },
      "source": [
        "kernels = tf.reshape(tf.transpose(W_conv1, perm=[2, 3, 0,1]),[32, -1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyVcnIkhRF9b"
      },
      "source": [
        "!wget --output-document utils1.py http://deeplearning.net/tutorial/code/utils.py\n",
        "import utils1\n",
        "import imp\n",
        "imp.reload(utils1)\n",
        "from utils1 import tile_raster_images\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "image = Image.fromarray(tile_raster_images(kernels.numpy(), img_shape=(5, 5) ,tile_shape=(4, 8), tile_spacing=(1, 1)))\n",
        "### Plot image\n",
        "plt.rcParams['figure.figsize'] = (18.0, 18.0)\n",
        "imgplot = plt.imshow(image)\n",
        "imgplot.set_cmap('gray')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0HvXLoDRF9g"
      },
      "source": [
        "¿Quiere ver la salida de una imagen pasando por la primera capa de convolución?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KxvZ5BIRF9h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "9f8bd61d-dacf-4a22-984c-652d9d2a5185"
      },
      "source": [
        "import numpy as np\n",
        "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
        "sampleimage = [x_image_train[0]]\n",
        "plt.imshow(np.reshape(sampleimage,[28,28]), cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa3eee924a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auNExEfRRF9j"
      },
      "source": [
        "#ActivatedUnits = sess.run(convolve1,feed_dict={x:np.reshape(sampleimage,[1,784],order='F'),keep_prob:1.0})\n",
        "keep_prob=1.0\n",
        "ActivatedUnits = convolve1(sampleimage)\n",
        "                           \n",
        "filters = ActivatedUnits.shape[3]\n",
        "plt.figure(1, figsize=(20,20))\n",
        "n_columns = 6\n",
        "n_rows = np.math.ceil(filters / n_columns) + 1\n",
        "for i in range(filters):\n",
        "    plt.subplot(n_rows, n_columns, i+1)\n",
        "    plt.title('Filter ' + str(i))\n",
        "    plt.imshow(ActivatedUnits[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vChUXA_eRF9n"
      },
      "source": [
        "¿Qué pasa con la segunda capa de convolución?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Uf3r-XRF9n"
      },
      "source": [
        "#ActivatedUnits = sess.run(convolve2,feed_dict={x:np.reshape(sampleimage,[1,784],order='F'),keep_prob:1.0})\n",
        "ActivatedUnits = convolve2(sampleimage)\n",
        "filters = ActivatedUnits.shape[3]\n",
        "plt.figure(1, figsize=(20,20))\n",
        "n_columns = 8\n",
        "n_rows = np.math.ceil(filters / n_columns) + 1\n",
        "for i in range(filters):\n",
        "    plt.subplot(n_rows, n_columns, i+1)\n",
        "    plt.title('Filter ' + str(i))\n",
        "    plt.imshow(ActivatedUnits[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}