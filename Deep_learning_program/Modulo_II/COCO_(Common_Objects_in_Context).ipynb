{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8EYyIluD2oqBkqq3UwQFu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCaballerot/Deep_learning_program/blob/main/Deep_learning_program/Modulo_II/COCO_(Common_Objects_in_Context).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center><h1>COCO (Common Objects in Context)</h1></center>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZIxfJjnkVpBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://cocodataset.org/images/coco-examples.jpg\" width=\"800\" height=\"300\">\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "MNhE6ldoV0NP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introducción\n"
      ],
      "metadata": {
        "id": "d2yphYkpXOei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 ¿Qué es el Dataset COCO?\n",
        "\n",
        "COCO (Common Objects in Context) es un dataset a gran escala diseñado para tareas de visión por computadora, como la detección de objetos, segmentación y descripción de imágenes. Fue creado para proporcionar una fuente de datos diversa y rica que represente objetos en escenas cotidianas. COCO incluye más de 330,000 imágenes, de las cuales más de 200,000 están anotadas con cuadros delimitadores (bounding boxes), segmentos y puntos clave. Hay un total de 80 clases de objetos en el dataset, que van desde personas y animales hasta objetos como sillas, bicicletas, y señales de tráfico.\n"
      ],
      "metadata": {
        "id": "GH5uydvUXRXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Objetivo del Notebook\n",
        "\n",
        "El objetivo de este notebook es desarrollar y entrenar un modelo de red neuronal convolucional (CNN) utilizando un modelo preentrenado (ResNet50) para realizar la tarea de detección de objetos en imágenes del dataset COCO. El notebook incluye pasos para la preparación de los datos, entrenamiento del modelo, evaluación del rendimiento, y visualización de las predicciones realizadas por el modelo sobre un conjunto de validación.\n"
      ],
      "metadata": {
        "id": "A93k8arzXXL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tabla de Contenidos\n",
        "\n",
        "\n",
        "1. <a>Introducción</a>\n",
        "2. <a>Preparación de los Datos</a>\n",
        "3. <a>Construcción del Modelo</a>\n",
        "4. <a>Entrenamiento del Modelo</a>\n",
        "5. <a>Evaluación del Modelo</a>\n",
        "6. <a>Visualización de Resultados</a>\n",
        "7. <a>Conclusiones</a>\n",
        "\n"
      ],
      "metadata": {
        "id": "nIv6imCsXcm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preparación de los Datos\n"
      ],
      "metadata": {
        "id": "tqFDB4yjX3zR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instala las librerías necesarias:"
      ],
      "metadata": {
        "id": "jwiK8WxAPpkf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWug9rltPUib"
      },
      "outputs": [],
      "source": [
        "!pip install pycocotools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Carga de las Anotaciones y las Imágenes\n",
        "En esta sección, se carga el dataset COCO, incluyendo las imágenes de validación y sus respectivas anotaciones. Se utiliza la librería pycocotools para manipular las anotaciones en formato JSON, extrayendo información relevante como los cuadros delimitadores y las etiquetas de cada objeto en la imagen."
      ],
      "metadata": {
        "id": "iaRqoTXaYAaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descarga una muestra del COCO Dataset (por ejemplo, las imágenes de validación):\n",
        "\n"
      ],
      "metadata": {
        "id": "iI-8FCkGPnx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga las imágenes de validación\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "\n",
        "# Descarga las anotaciones correspondientes\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "\n",
        "# Descomprime los archivos\n",
        "!unzip val2017.zip -d ./coco/\n",
        "!unzip annotations_trainval2017.zip -d ./coco/\n"
      ],
      "metadata": {
        "id": "2lvi5ZsSPh-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga y visualiza una imagen junto con sus anotaciones:\n",
        "\n"
      ],
      "metadata": {
        "id": "3aUymA6YPwVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "CM0WVGh2P2hg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configura la ruta a las imágenes y anotaciones\n",
        "data_dir = './coco'\n",
        "image_dir = os.path.join(data_dir, 'val2017')\n",
        "ann_file = os.path.join(data_dir, 'annotations/instances_val2017.json')\n",
        "\n",
        "# Carga las anotaciones\n",
        "coco = COCO(ann_file)\n"
      ],
      "metadata": {
        "id": "xlt722yaPvYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Selecciona una imagen al azar\n",
        "image_id = coco.getImgIds()[4]\n",
        "image_info = coco.loadImgs(image_id)[0]\n",
        "image_path = os.path.join(image_dir, image_info['file_name'])\n",
        "\n",
        "# Carga la imagen\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Carga las anotaciones correspondientes\n",
        "ann_ids = coco.getAnnIds(imgIds=image_info['id'])\n",
        "anns    = coco.loadAnns(ann_ids)\n",
        "\n",
        "# Visualiza la imagen y sus anotaciones\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(image)\n",
        "\n",
        "# Añade los cuadros delimitadores\n",
        "for ann in anns:\n",
        "    bbox = ann['bbox']\n",
        "    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hG_jamOLP7T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualiza la imagen y sus anotaciones\n",
        "fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "ax.imshow(image)\n",
        "\n",
        "# Añade los cuadros delimitadores y etiquetas\n",
        "for ann in anns:\n",
        "    bbox = ann['bbox']\n",
        "    category_id = ann['category_id']\n",
        "    category_name = coco.loadCats(category_id)[0]['name']\n",
        "\n",
        "    # Dibuja el cuadro delimitador\n",
        "    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    # Añade el texto con la etiqueta\n",
        "    plt.text(bbox[0], bbox[1] - 10, category_name, color='white', fontsize=12, backgroundcolor='red')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TGpG--gRRRr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Preprocesamiento y Estructuración de los Datos\n",
        "Las imágenes se redimensionan a un tamaño fijo de 224x224 píxeles, y se asegura que todas las imágenes estén en formato RGB (3 canales). Se crean etiquetas en formato de vectores binarios, donde cada posición del vector representa una clase en el dataset COCO.\n",
        "\n"
      ],
      "metadata": {
        "id": "31rLrxoPYTKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "TK_j5yl6SqOG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configura la ruta a las imágenes y anotaciones\n",
        "data_dir = './coco'\n",
        "image_dir = os.path.join(data_dir, 'val2017')\n",
        "ann_file = os.path.join(data_dir, 'annotations/instances_val2017.json')\n",
        "\n",
        "# Carga las anotaciones\n",
        "coco = COCO(ann_file)\n",
        "\n",
        "# Obtén las categorías\n",
        "categories = coco.loadCats(coco.getCatIds())\n",
        "category_names = [cat['name'] for cat in categories]\n",
        "num_classes = len(category_names)\n",
        "category_id_to_index = {cat['id']: idx for idx, cat in enumerate(categories)}\n"
      ],
      "metadata": {
        "id": "Ho8xv2d7S5GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar imágenes y sus etiquetas\n",
        "image_ids = coco.getImgIds()\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for image_id in image_ids:\n",
        "    image_info = coco.loadImgs(image_id)[0]\n",
        "    image_path = os.path.join(image_dir, image_info['file_name'])\n",
        "    image = Image.open(image_path).resize((224, 224))\n",
        "\n",
        "    # Convertir a color (3 canales) si es necesario\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    images.append(np.array(image))\n",
        "\n",
        "    # Extraer las categorías de los objetos en la imagen\n",
        "    ann_ids = coco.getAnnIds(imgIds=image_info['id'])\n",
        "    anns = coco.loadAnns(ann_ids)\n",
        "    image_labels = np.zeros(num_classes)\n",
        "    for ann in anns:\n",
        "        cat_id = ann['category_id']\n",
        "        if cat_id in category_id_to_index:\n",
        "            image_labels[category_id_to_index[cat_id]] = 1\n",
        "    labels.append(image_labels)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "LgmYVhHMS7aH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir listas a arrays de NumPy\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "fPC_wMscT2jr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 División del Dataset\n",
        "Se divide el dataset en un conjunto de entrenamiento y un conjunto de validación, utilizando una proporción del 80% para entrenamiento y el 20% para validación."
      ],
      "metadata": {
        "id": "skRKnHBlYW_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# División del conjunto de datos en entrenamiento y validación\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(images,\n",
        "                                                  labels,\n",
        "                                                  test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "ikv_QhVlSoP2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "Ym-cqdYQUtNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val.shape"
      ],
      "metadata": {
        "id": "7AZUcwCFU0i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "tcpDgiyAU39e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utilizan generadores de datos de Keras (ImageDataGenerator) para realizar aumentación de datos durante el entrenamiento, lo que ayuda a mejorar la capacidad de generalización del modelo."
      ],
      "metadata": {
        "id": "sB0I8jMMY7DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesamiento de los datos\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, rotation_range=20)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow(X_train, y_train, batch_size=500)\n",
        "val_generator = val_datagen.flow(X_val, y_val, batch_size=500)\n"
      ],
      "metadata": {
        "id": "yLqWuQHcU9K_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "OyLXBct8dlWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suponiendo que y_train es un array de dummies (One-hot encoded)\n",
        "class_counts = np.sum(y_train, axis=0)  # Sumamos los valores en cada columna\n",
        "\n",
        "# Imprimir la cantidad de ejemplos por clase\n",
        "for i, count in enumerate(class_counts):\n",
        "    print(f\"Clase '{category_names[i]}': {count} ejemplos\")\n",
        "\n",
        "# Gráfico de barras para visualizar el balance de clases\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(len(class_counts)), class_counts, tick_label=category_names)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Distribución de clases en y_train (One-hot encoded)\")\n",
        "plt.xlabel(\"Clases\")\n",
        "plt.ylabel(\"Cantidad de ejemplos\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-8QFU8gNew4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Construcción del Modelo\n"
      ],
      "metadata": {
        "id": "i40PdtEiYgBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4.1 Uso de un Modelo Preentrenado\n",
        "\n",
        "Se utiliza el modelo ResNet50 preentrenado en el dataset ImageNet como base. Este modelo ya ha aprendido a detectar características importantes en imágenes y servirá como punto de partida para la tarea de detección de objetos."
      ],
      "metadata": {
        "id": "-oreEsEFYa8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construcción del modelo CNN con ResNet50\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n"
      ],
      "metadata": {
        "id": "r7aApnQPVI6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Configuración de Capas Adicionales\n",
        "\n",
        "Se añaden capas adicionales al modelo base, incluyendo una capa de GlobalAveragePooling2D, una capa densa (fully connected) con activación ReLU, y una capa de salida con activación sigmoide para realizar la clasificación multietiqueta."
      ],
      "metadata": {
        "id": "8Jp-0j3fYm5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "gUUm5RKqVMC7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Compilación del Modelo\n",
        "\n",
        "Se compila el modelo utilizando el optimizador Adam y la función de pérdida binary_crossentropy, que es adecuada para tareas de clasificación multietiqueta."
      ],
      "metadata": {
        "id": "8ReAQR1QYq6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "zPArH8n9VQAt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo se entrena utilizando los datos del conjunto de entrenamiento, con 10 épocas de entrenamiento. Se monitorea el rendimiento del modelo en el conjunto de validación después de cada época.\n",
        "\n"
      ],
      "metadata": {
        "id": "zFfXJzvgYzQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Entrenamiento del modelo\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_generator,\n",
        "                    verbose=1)\n"
      ],
      "metadata": {
        "id": "XEuIoOkLVTUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluación del Modelo\n"
      ],
      "metadata": {
        "id": "6fIFIobyZN0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Evaluación del Modelo\n",
        "Se evalúa el rendimiento del modelo en el conjunto de validación utilizando las métricas de pérdida y precisión.\n",
        "\n"
      ],
      "metadata": {
        "id": "aOT55SZXZS9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluación del modelo\n",
        "val_loss, val_accuracy = model.evaluate(val_generator, verbose=1)\n",
        "print(f\"Validation Loss: {val_loss}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n"
      ],
      "metadata": {
        "id": "h_2CbVXsVapt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Cálculo de Métricas de Rendimiento\n",
        "Se genera un informe de clasificación (classification_report) que incluye métricas como precisión, recall y F1-score para cada clase."
      ],
      "metadata": {
        "id": "XZHPzgFRZYio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones en el conjunto de validación\n",
        "y_pred = model.predict(val_generator)\n",
        "\n",
        "# Calcular la métrica de rendimiento\n",
        "y_val_labels = np.argmax(y_val, axis=1)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Encontrar las clases presentes en el conjunto de validación\n",
        "present_classes = np.unique(y_val_labels)\n",
        "\n",
        "# Filtrar los nombres de las categorías correspondientes a las clases presentes\n",
        "filtered_category_names = [category_names[i] for i in present_classes]\n",
        "\n",
        "# Calcular el classification_report solo para las clases presentes\n",
        "print(classification_report(y_val_labels, y_pred_labels, target_names=filtered_category_names))\n"
      ],
      "metadata": {
        "id": "6kxI9pkOWko7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualización de Resultados\n"
      ],
      "metadata": {
        "id": "wwcuok_BZgW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Visualización de Predicciones\n",
        "Se seleccionan aleatoriamente algunas imágenes del conjunto de validación y se muestran junto con las predicciones del modelo. Las etiquetas predichas se muestran sobre los cuadros delimitadores correspondientes en la imagen."
      ],
      "metadata": {
        "id": "RDhzOIKwZkyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "def plot_predictions(image, predictions, threshold=0.5):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Dibujar recuadros alrededor de las predicciones con una probabilidad mayor al umbral\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i] > threshold:\n",
        "            label = category_names[i]\n",
        "            prob = predictions[i]\n",
        "\n",
        "            # Aquí puedes definir las coordenadas del cuadro delimitador.\n",
        "            # Para este ejemplo, usaré coordenadas arbitrarias.\n",
        "            bbox = [50, 50 + i * 40, 150, 100]  # Coordenadas ficticias para ilustración\n",
        "\n",
        "            # Crear un rectángulo (recuadro) en la imagen\n",
        "            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "\n",
        "            # Añadir la etiqueta y la probabilidad encima del recuadro\n",
        "            plt.text(bbox[0], bbox[1] - 10, f\"{label}: {prob:.2f}\", color='white', fontsize=12, backgroundcolor='red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Selección de imágenes del conjunto de validación para mostrar predicciones\n",
        "for i in range(5):\n",
        "    index = np.random.randint(0, len(X_val))\n",
        "    image = X_val[index]\n",
        "    predictions = y_pred[index]\n",
        "\n",
        "    plot_predictions(image, predictions)\n"
      ],
      "metadata": {
        "id": "j3Cla7PNWpPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusiones\n",
        "\n"
      ],
      "metadata": {
        "id": "7X5YtYhDZqqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- El modelo basado en ResNet50 preentrenado mostró un rendimiento aceptable en la detección de múltiples clases de objetos, pero algunas clases fueron más difíciles de predecir correctamente.\n",
        "\n",
        "- La detección de múltiples objetos en una imagen es compleja, y el modelo puede confundir clases similares. Aumentar la diferenciación de características podría mejorar el rendimiento.\n",
        "\n",
        "\n",
        "- La normalización y conversión consistente de imágenes fueron esenciales para el entrenamiento exitoso del modelo.\n",
        "\n",
        "\n",
        "- El modelo identificó correctamente varios objetos, pero también mostró errores significativos en algunos casos. Hay margen para mejorar, especialmente mediante la afinación de hiperparámetros o la integración de enfoques más avanzados.\n",
        "\n",
        "\n",
        "- Se recomienda afinar los hiperparámetros, incrementar la diversidad del dataset, y explorar modelos más avanzados o técnicas de ensamblaje para mejorar el rendimiento."
      ],
      "metadata": {
        "id": "hm_hTqKQZu_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Gracias por completar este laboratorio!"
      ],
      "metadata": {
        "id": "qeUqbthrZ5p_"
      }
    }
  ]
}