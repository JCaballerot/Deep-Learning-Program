{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYiD8NaXUocTcANRcyjpee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCaballerot/Deep_learning_program/blob/main/Topicos_avanzados/concatenacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\"><font size=\"5\">FASHION MNIST</font></h1>\n"
      ],
      "metadata": {
        "id": "SbkVbrklNAeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://www.researchgate.net/publication/346405197/figure/fig3/AS:962581560848384@1606508736352/Examples-of-Fashion-MNIST-dataset.ppm\" width=\"800\" height=\"300\">\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "JpnZsHToQUP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este laboratorio esta construido para construir y entrenar modelos de redes neuronales convolucionales (CNN) usando el dataset Fashion MNIST, un conjunto de datos popular para el aprendizaje automático que consiste en imágenes de artículos de moda. A continuación, se detallan los pasos y componentes clave del script:\n",
        "\n"
      ],
      "metadata": {
        "id": "aZRdlQwXQ3-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Carga y Preprocesamiento de Datos\n"
      ],
      "metadata": {
        "id": "QIRwfXc5RGTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero, importamos el dataset Fashion MNIST utilizando la biblioteca Keras. Este dataset incluye 60,000 imágenes para entrenamiento y 10,000 imágenes para pruebas, cada una de 28x28 píxeles, en escala de grises."
      ],
      "metadata": {
        "id": "kROhVRb2RJc6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKUXfe1BLNxa"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Cargar el dataset Fashion MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Redimensionar las imágenes y normalizar\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# Convertir los labels a one-hot encoding\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Construcción de Modelos de Redes Neuronales Convolucionales (CNN)\n"
      ],
      "metadata": {
        "id": "JmziCG9eRQh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Modelo sin Concatenación**\n",
        "\n",
        "Primero, construimos un modelo de CNN básico usando la clase Sequential de Keras, que apila capas en una secuencia lineal. Este modelo incluye tres capas convolucionales Conv2D para la extracción de características, seguidas por capas de MaxPooling2D para reducir la dimensionalidad, y finaliza con capas Dense para la clasificación."
      ],
      "metadata": {
        "id": "_OF0YJy8RUag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Modelo sin concatenación\n",
        "model_sin_concat = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_sin_concat.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_sin_concat.summary()\n"
      ],
      "metadata": {
        "id": "UNI6awcxLWvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo con Concatenación**\n",
        "\n",
        "Luego, construimos un modelo más complejo que utiliza una arquitectura de red con ramas. Este modelo toma la misma entrada y aplica dos series de operaciones en paralelo (denominadas ramas), para luego concatenar sus salidas y continuar con capas densas hasta la clasificación final."
      ],
      "metadata": {
        "id": "me4qM7uHRcWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "\n",
        "# Entrada\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "\n",
        "# Rama 1\n",
        "x1 = Conv2D(32, (3, 3), padding='same')(input_img)\n",
        "x1 = BatchNormalization()(x1)\n",
        "x1 = Activation('relu')(x1)\n",
        "x1 = MaxPooling2D((2, 2))(x1)\n",
        "\n",
        "# Rama 2\n",
        "x2 = Conv2D(64, (3, 3), padding='same')(input_img)\n",
        "x2 = BatchNormalization()(x2)\n",
        "x2 = Activation('relu')(x2)\n",
        "x2 = MaxPooling2D((2, 2))(x2)\n",
        "\n",
        "# Concatenación\n",
        "concatenated = concatenate([x1, x2])\n",
        "\n",
        "# Reducción y salida\n",
        "x = Flatten()(concatenated)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "output = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Modelo con concatenación\n",
        "model_con_concat = Model(inputs=input_img, outputs=output)\n",
        "\n",
        "model_con_concat.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_con_concat.summary()\n"
      ],
      "metadata": {
        "id": "fucDZPgfLhVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Entrenamiento y Evaluación"
      ],
      "metadata": {
        "id": "ZyHX7IcxRhiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento y Evaluación\n",
        "Ambos modelos se entrenan usando imágenes de entrenamiento con sus respectivos labels, especificando el número de epochs y el tamaño del batch. También se separa un 20% de los datos de entrenamiento para validar el modelo durante el entrenamiento.\n",
        "\n",
        "Después del entrenamiento, evaluamos cada modelo en el conjunto de prueba para comparar su desempeño, observando las métricas de pérdida y precisión."
      ],
      "metadata": {
        "id": "6qDC7w1YRhG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del modelo sin concatenación\n",
        "history_sin_concat = model_sin_concat.fit(train_images, train_labels,\n",
        "                                          epochs=10,\n",
        "                                          batch_size=64,\n",
        "                                          validation_split=0.2)\n",
        "\n",
        "# Entrenamiento del modelo con concatenación\n",
        "history_con_concat = model_con_concat.fit(train_images, train_labels,\n",
        "                                          epochs=10,\n",
        "                                          batch_size=64,\n",
        "                                          validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4cpwG2cLubL",
        "outputId": "49d6e086-9b26-4464-8c1b-84f9ab59cbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 50s 65ms/step - loss: 0.5881 - accuracy: 0.7842 - val_loss: 0.4141 - val_accuracy: 0.8510\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 43s 58ms/step - loss: 0.3602 - accuracy: 0.8696 - val_loss: 0.3465 - val_accuracy: 0.8733\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 46s 62ms/step - loss: 0.3103 - accuracy: 0.8879 - val_loss: 0.3071 - val_accuracy: 0.8907\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 0.2794 - accuracy: 0.8978 - val_loss: 0.2855 - val_accuracy: 0.8985\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 0.2536 - accuracy: 0.9064 - val_loss: 0.2705 - val_accuracy: 0.9022\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 45s 59ms/step - loss: 0.2334 - accuracy: 0.9127 - val_loss: 0.2643 - val_accuracy: 0.9052\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 42s 57ms/step - loss: 0.2170 - accuracy: 0.9199 - val_loss: 0.2719 - val_accuracy: 0.9023\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 49s 65ms/step - loss: 0.1998 - accuracy: 0.9268 - val_loss: 0.2523 - val_accuracy: 0.9087\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 0.1866 - accuracy: 0.9309 - val_loss: 0.2470 - val_accuracy: 0.9126\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 0.1699 - accuracy: 0.9364 - val_loss: 0.2523 - val_accuracy: 0.9103\n",
            "Epoch 1/10\n",
            "750/750 [==============================] - 112s 148ms/step - loss: 0.4788 - accuracy: 0.8416 - val_loss: 0.3677 - val_accuracy: 0.8648\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 113s 151ms/step - loss: 0.2966 - accuracy: 0.8950 - val_loss: 0.2979 - val_accuracy: 0.8952\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 113s 151ms/step - loss: 0.2571 - accuracy: 0.9066 - val_loss: 0.2940 - val_accuracy: 0.8969\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 115s 153ms/step - loss: 0.2283 - accuracy: 0.9167 - val_loss: 0.3104 - val_accuracy: 0.8920\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 112s 149ms/step - loss: 0.2081 - accuracy: 0.9248 - val_loss: 0.3102 - val_accuracy: 0.8958\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 110s 146ms/step - loss: 0.1937 - accuracy: 0.9305 - val_loss: 0.2941 - val_accuracy: 0.9033\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 115s 153ms/step - loss: 0.1767 - accuracy: 0.9354 - val_loss: 0.3092 - val_accuracy: 0.9020\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 137s 183ms/step - loss: 0.1651 - accuracy: 0.9396 - val_loss: 0.2870 - val_accuracy: 0.9068\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 156s 208ms/step - loss: 0.1536 - accuracy: 0.9444 - val_loss: 0.3075 - val_accuracy: 0.9088\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 145s 193ms/step - loss: 0.1365 - accuracy: 0.9501 - val_loss: 0.3355 - val_accuracy: 0.8954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del modelo sin concatenación en el conjunto de prueba\n",
        "test_loss_sin_concat, test_acc_sin_concat = model_sin_concat.evaluate(test_images, test_labels)\n",
        "\n",
        "# Evaluación del modelo con concatenación en el conjunto de prueba\n",
        "test_loss_con_concat, test_acc_con_concat = model_con_concat.evaluate(test_images, test_labels)\n",
        "\n",
        "print(\"Modelo sin concatenación - Pérdida: {:.4f}, Precisión: {:.4f}\".format(test_loss_sin_concat, test_acc_sin_concat))\n",
        "print(\"Modelo con concatenación - Pérdida: {:.4f}, Precisión: {:.4f}\".format(test_loss_con_concat, test_acc_con_concat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWWiCD7oL10x",
        "outputId": "b0f2a6a9-411d-4fac-a5bf-b763b70fa172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 11ms/step - loss: 0.2589 - accuracy: 0.9121\n",
            "313/313 [==============================] - 6s 21ms/step - loss: 0.3531 - accuracy: 0.8945\n",
            "Modelo sin concatenación - Pérdida: 0.2589, Precisión: 0.9121\n",
            "Modelo con concatenación - Pérdida: 0.3531, Precisión: 0.8945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La concatenación en modelos de redes neuronales, como se muestra, representa una técnica poderosa para mejorar la capacidad del modelo de aprender y combinar características a diferentes niveles de abstracción. Al fusionar las salidas de múltiples ramas de convolución y agrupación, este enfoque permite al modelo capturar una variedad más rica de patrones y detalles en las imágenes, lo que puede conducir a un rendimiento superior en tareas de clasificación.\n"
      ],
      "metadata": {
        "id": "Zm34ML7QR2p0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Gracias por completar este laboratorio!"
      ],
      "metadata": {
        "id": "T4QZzyIDR_N4"
      }
    }
  ]
}